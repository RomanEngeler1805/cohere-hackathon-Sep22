{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOOqpIRfBsoBzZ60trwanaJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RomanEngeler1805/cohere-hackathon-Sep22/blob/main/Cohere_Embed_Analyse_Streamlit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cohere Analyse\n",
        "This script has been developed during the *Cohere AI Hackathon #2* to make the Cohere embed endpoints better accessible. <br><br>\n",
        "\n",
        "It runs and displays all outputs within a streamlit app. It is meant for demonstration purposes with a longer latency to fire up streamlit.\n",
        "<br><br>\n",
        "\n",
        "It consists of four parts:\n",
        "<li>\n",
        "Data upload\n",
        "<li>\n",
        "Exploratory data analysis (EDA)\n",
        "<li>\n",
        "Cluster analysis\n",
        "<li>\n",
        "Semantic search\n",
        "\n",
        "<br>\n",
        "\n",
        "TODO: search for TODO keywords and insert valid cohere api key and ngrok key.\n",
        "\n",
        "<br>\n",
        "Make sure to run all cells sequentially and not skip any section<br>\n",
        "A CPU is enough to run it as all the heavy lifting is done on Cohere's side\n"
      ],
      "metadata": {
        "id": "LtUpzjguG3qp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install & Imports"
      ],
      "metadata": {
        "id": "sqc3JmuGiJF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cohere umap-learn altair annoy bertopic streamlit pyngrok==4.1.1 -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfwn76pbG2T6",
        "outputId": "974e06d2-0268-42d3-a296-03412052323e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 88 kB 4.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 647 kB 17.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 76 kB 1.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 9.1 MB 43.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 12.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 5.2 MB 7.8 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 636 kB 56.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 85 kB 5.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 54.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 47.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 120 kB 62.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 43.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 164 kB 61.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 50.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 78 kB 8.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 235 kB 62.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 181 kB 38.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 51 kB 8.2 MB/s \n",
            "\u001b[?25h  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for cohere (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for hdbscan (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Streamlit Scripts"
      ],
      "metadata": {
        "id": "vzVf07-wiZ9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile helper.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import cohere\n",
        "import umap\n",
        "import warnings\n",
        "from sklearn.cluster import KMeans\n",
        "from bertopic._ctfidf import ClassTFIDF\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "from typing import Tuple\n",
        "\n",
        "# TODO: insert valid api key\n",
        "api_key = None\n",
        "co = cohere.Client(api_key)\n",
        "title = 'Some title needs to be inputted here'\n",
        "\n",
        "@st.cache(allow_output_mutation=True)\n",
        "# Load & Prepare dataset\n",
        "def get_dataset(df: pd.DataFrame, text: str, title: str, max_length: int=100) -> pd.DataFrame:\n",
        "  '''\n",
        "  inputs:\n",
        "  - df: dataframe of data\n",
        "  - text: name of text column\n",
        "  - title: name of title column\n",
        "  - max_length: parameter to limit length for the sake of speed\n",
        "\n",
        "  outputs:\n",
        "  - df: dataframe with 'text' and 'title' column\n",
        "  '''\n",
        "  df.rename(columns={text: 'text', title: 'title'}, inplace=True)\n",
        "  df = df[['title', 'text']]\n",
        "  max_length = min(max_length, df.shape[0])\n",
        "  df = df.head(max_length)\n",
        "  return df\n",
        "\n",
        "@st.cache(allow_output_mutation=True)\n",
        "def get_embeddings(df: pd.DataFrame) -> Tuple[list, list]:\n",
        "  '''\n",
        "  input:\n",
        "  - df: dataframe with 'text' column\n",
        "\n",
        "  output:\n",
        "  - embeds: cohere embedding\n",
        "  - umap_embeds: umap embeddings -> dimensionality reduction technique\n",
        "  '''\n",
        "  embeds = co.embed(texts=list(df['text']),\n",
        "                  model='medium',\n",
        "                  truncate='LEFT').embeddings\n",
        "  reducer = umap.UMAP(n_neighbors=100) \n",
        "  umap_embeds = reducer.fit_transform(embeds)\n",
        "  return (embeds, umap_embeds)\n",
        "\n",
        "@st.cache(allow_output_mutation=True)\n",
        "def get_keywords(df: pd.DataFrame, n_clusters: int=8) -> pd.DataFrame:\n",
        "  '''\n",
        "  inputs:\n",
        "  - df: dataframe with columns ('text', 'title', 'embeds', 'x', 'y')\n",
        "  - n_clusters: number of clusters in k-means\n",
        "\n",
        "  outputs:\n",
        "  - df: dataframe with columns ('text', 'topic', 'embeds', 'x', 'y', 'cluster', 'keywords')\n",
        "  - chart-title\n",
        "  '''\n",
        "\n",
        "  # k-means clustering\n",
        "  kmeans_model = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "  classes = kmeans_model.fit_predict(list(df['embeds'].values))\n",
        "\n",
        "  # get keywords from each cluster\n",
        "  # - group documents by cluster assignment\n",
        "  # - get tf-id for the topic words in each cluster \n",
        "  documents =  df['title']\n",
        "  documents = pd.DataFrame({\"Document\": documents,\n",
        "                            \"ID\": range(len(documents)),\n",
        "                            \"Topic\": None})\n",
        "  documents['Topic'] = classes\n",
        "  documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
        "  count_vectorizer = CountVectorizer(stop_words=\"english\").fit(documents_per_topic.Document)\n",
        "  count = count_vectorizer.transform(documents_per_topic.Document)\n",
        "  words = count_vectorizer.get_feature_names()\n",
        "  ctfidf = ClassTFIDF().fit_transform(count).toarray()\n",
        "  words_per_class = {label: [words[index] for index in ctfidf[label].argsort()[-10:]] for label in documents_per_topic.Topic}\n",
        "\n",
        "  # add cluster assignment and keywords per cluster to dataframe\n",
        "  df['cluster'] = classes\n",
        "  df['keywords'] = df['cluster'].map(lambda topic_num: \", \".join(np.array(words_per_class[topic_num])[:]))\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_a0hsW1iZX8",
        "outputId": "5e2c23b4-7dca-4b18-fc27-482177f212bc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing helper.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile semantic_search.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from annoy import AnnoyIndex\n",
        "from helper import get_dataset, get_embeddings, get_keywords\n",
        "from typing import Tuple\n",
        "\n",
        "@st.cache(allow_output_mutation=True)\n",
        "def search(df: pd.DataFrame, query: str, n_relevantDocs: int=20) -> pd.DataFrame:\n",
        "  '''\n",
        "  inputs:\n",
        "  - df: dataframe with embeds column\n",
        "  - query: search query\n",
        "  - n_relevantDocs: number of documents to return for query\n",
        "\n",
        "  outputs:\n",
        "  - df: dataframe with additional collumn 'relevance' in [0, 1]\n",
        "  '''\n",
        "\n",
        "  # query and embedding\n",
        "  temp_dict = {'text': query}\n",
        "  df_query = pd.DataFrame(temp_dict, index=[0])\n",
        "\n",
        "  # embed query\n",
        "  query_embed, query_umap_embed = get_embeddings(df_query)\n",
        "\n",
        "  # create search index\n",
        "  embeds =  np.array(list(df['embeds'].values))\n",
        "\n",
        "  search_index = AnnoyIndex(embeds.shape[1], 'angular')\n",
        "  # Add all the vectors to the search index\n",
        "  for i in range(len(embeds)):\n",
        "      search_index.add_item(i, embeds[i])\n",
        "\n",
        "  search_index.build(10) # 10 trees\n",
        "  search_index.save('test.ann')\n",
        "\n",
        "  # Retrieve the nearest neighbors\n",
        "  similar_item_ids = search_index.get_nns_by_vector(query_embed[0],\n",
        "                                                    n_relevantDocs,\n",
        "                                                    include_distances=True)\n",
        "  # Format the results\n",
        "  results = pd.DataFrame(data={'texts': df.iloc[similar_item_ids[0]]['text'],\n",
        "                              'distance': similar_item_ids[1]})\n",
        "\n",
        "  # dataframe for plotting -> (x, y, relevance) with relevance 0 or 1\n",
        "  relevant_docs = []\n",
        "\n",
        "  for k in range(len(df)):\n",
        "    if k in similar_item_ids[0]:\n",
        "      relevant_docs.append(1)\n",
        "    else:\n",
        "      relevant_docs.append(0)\n",
        "\n",
        "  df_relevantDocs = pd.DataFrame(relevant_docs, columns=['relevance'])\n",
        "\n",
        "  df = df.join(df_relevantDocs);\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rABq0ZBgiGUG",
        "outputId": "e5bf2f10-e60f-4310-f6b6-6f3b4f67c770"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing semantic_search.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import umap\n",
        "import altair as alt\n",
        "import warnings\n",
        "from PIL import Image\n",
        "import requests\n",
        "from sklearn.cluster import KMeans\n",
        "from bertopic._ctfidf import ClassTFIDF\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from typing import Tuple\n",
        "import streamlit as st\n",
        "from helper import get_dataset, get_embeddings, get_keywords\n",
        "from semantic_search import search\n",
        "from scipy.spatial.distance import cdist\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# work-around to not trigger get_embeddings at every tab change\n",
        "file_name = None\n",
        "\n",
        "def main():\n",
        "\n",
        "  df = pd.DataFrame({'title': [], 'text': []})\n",
        "  # title\n",
        "  col1, _, col2 = st.columns([1,1,15])\n",
        "\n",
        "  image_cohere = Image.open(requests.get('https://avatars.githubusercontent.com/u/54850923?s=280&v=4', stream=True).raw)\n",
        "  col1.image(image_cohere, width=80)\n",
        "  #\n",
        "  col2.title('Analyze')\n",
        "\n",
        "  with st.expander(\"How this works.\", expanded=False):\n",
        "    st.write(\n",
        "      \"\"\"     \n",
        "      Embeddings are hard to vizualize. Analyze makes it a breeze.\n",
        "      1. Go ahead and upload a csv file that you want to examine.\n",
        "          The csv file needs to have atleast 2 columns:\n",
        "          - The first column being shorter text - a title for example.\n",
        "          - The second column being the longer text - the body of the text for example.\n",
        "      2. You then have 3 options:\n",
        "          - EDA: Get an overview of the file and get some general exploratory data analysis.\n",
        "          - Cluster: Do some cluster analysis, with keywords generated from the body of the text and using the titles.\n",
        "          - Search: Query the data and retrieve the closest match.\n",
        "\n",
        "      To make sure this app works quickly, we only capture the first 500 lines of text.     \n",
        "      \"\"\"\n",
        "    )\n",
        "    st.markdown('')\n",
        "  \n",
        "  # file upload\n",
        "  uploaded_file = st.file_uploader(\"Choose a file\")\n",
        "\n",
        "  if (uploaded_file is not None):\n",
        "    df = pd.read_csv(uploaded_file, usecols=[5, 6]) # TODO: remove hard coding\n",
        "\n",
        "    df.columns = ['text', 'title']\n",
        "    df = df.head(100)\n",
        "    df = get_dataset(df, text='text', title='title')\n",
        "\n",
        "  #\n",
        "  app_mode = st.sidebar.selectbox('Task', ['Import', 'EDA', 'Cluster', 'Search'])\n",
        "\n",
        "  if app_mode == \"Import\":\n",
        "      st.markdown('')\n",
        "\n",
        "      st.markdown(\n",
        "          \"\"\"\n",
        "          <style>\n",
        "          [data-testid=\"stSidebar\"][aria-expanded=\"true\"] > div:first-child{\n",
        "              width: 350px\n",
        "          }\n",
        "          [data-testid=\"stSidebar\"][aria-expanded=\"false\"] > div:first-child{\n",
        "              width: 350px\n",
        "              margin-left: -350px\n",
        "          }\n",
        "          </style>\n",
        "          \"\"\",\n",
        "\n",
        "          unsafe_allow_html=True,\n",
        "      )\n",
        "      \n",
        "  elif app_mode == 'EDA':\n",
        "    st.sidebar.subheader(' Quick  Explore')\n",
        "    st.markdown(\"Tick the box on the side panel to explore the dataset.\")\n",
        "    if st.sidebar.checkbox('Basic Info'):\n",
        "        if st.sidebar.checkbox(\"Show Columns\"):\n",
        "            st.subheader('Show Columns List')\n",
        "            all_columns = df.columns.to_list()\n",
        "            st.write(all_columns)\n",
        "\n",
        "        if st.sidebar.checkbox('Overview'):\n",
        "            st.subheader('File contents')\n",
        "            st.write(df)\n",
        "            st.write(f'The number of lines is {df.shape[0]}. We will only process {min(500, df.shape[0])}')\n",
        "        if st.sidebar.checkbox('Missing Values?'):\n",
        "            st.subheader('Missing values')\n",
        "            st.write(df.isnull().sum())\n",
        "\n",
        "  elif app_mode == 'Cluster':\n",
        "    if 'embeds' not in df:\n",
        "      # get cohere and umap embeddings\n",
        "      embeds, umap_embeds = get_embeddings(df)\n",
        "      # store umap embeddings in dataframe for plotting\n",
        "      df['embeds'] = embeds\n",
        "      df['x'] = umap_embeds[:,0]\n",
        "      df['y'] = umap_embeds[:,1]\n",
        "\n",
        "    # number of clusters\n",
        "    low, med, high = 1, 8, 10\n",
        "\n",
        "    with st.expander(\"Help\", expanded=False):\n",
        "      st.write(\n",
        "          \"\"\"     \n",
        "          One of the ways to determine the optimal number of clusters, is to choose the number corresponding to an elbow.\n",
        "          \"\"\"\n",
        "      )\n",
        "      st.markdown(\"\")\n",
        "\n",
        "      # determine distortion as a function of the number of clusters in kmeans\n",
        "      distortions = []\n",
        "      nembeds = np.array(list(df['embeds'].values))\n",
        "      for k in range(low, high + 1):\n",
        "        km = KMeans(n_clusters=k)\n",
        "        km.fit(nembeds)\n",
        "        distortions.append(sum(np.min(cdist(nembeds, km.cluster_centers_,\n",
        "                                              'euclidean'), axis=1)) / nembeds.shape[0])  \n",
        "\n",
        "      fig = plt.figure(figsize=(10, 4))\n",
        "      plt.plot(range(low, high + 1), distortions, 'bx-')\n",
        "      plt.xlabel('Number of clusters')\n",
        "      plt.ylabel('Distortion')\n",
        "      plt.title('Determine the optimal number of clusters')\n",
        "      st.pyplot(fig)\n",
        "\n",
        "    # user can choose number of clusters\n",
        "    n_clusters = st.slider('Select number of clusters', low, high, med)\n",
        "    df = get_keywords(df, n_clusters=n_clusters)\n",
        "    selection = alt.selection_multi(fields=['keywords'], bind='legend')\n",
        "    chart = alt.Chart(df).transform_calculate(\n",
        "        url=alt.datum.id\n",
        "    ).mark_circle(size=60, stroke='#666', strokeWidth=1, opacity=0.3).encode(\n",
        "        x=#'x',\n",
        "        alt.X('x',\n",
        "            scale=alt.Scale(zero=False),\n",
        "            axis=alt.Axis(labels=False, ticks=False, domain=False)\n",
        "        ),\n",
        "        y=\n",
        "        alt.Y('y',\n",
        "            scale=alt.Scale(zero=False),\n",
        "            axis=alt.Axis(labels=False, ticks=False, domain=False)\n",
        "        ),\n",
        "        href='url:N',\n",
        "        color=alt.Color('keywords:N', \n",
        "                        legend=alt.Legend(columns=1, symbolLimit=0, labelFontSize=14)\n",
        "                      ),\n",
        "        opacity=alt.condition(selection, alt.value(1), alt.value(0.2)),\n",
        "        tooltip=['title', 'keywords', 'cluster']\n",
        "    ).properties(\n",
        "        width=800,\n",
        "        height=500\n",
        "    ).add_selection(\n",
        "        selection\n",
        "    ).configure_legend(labelLimit= 0).configure_view(\n",
        "        strokeWidth=0\n",
        "    ).configure(background=\"#FAFAFA\").properties(\n",
        "        title='K-Means clustering in 2D umap visualisation'\n",
        "    ).interactive()\n",
        "    st.altair_chart(chart, use_container_width=True)\n",
        "  \n",
        "  elif app_mode == 'Search':\n",
        "    if 'embeds' not in df:\n",
        "      # get cohere and umap embeddings\n",
        "      embeds, umap_embeds = get_embeddings(df)\n",
        "      # store umap embeddings in dataframe for plotting\n",
        "      df['embeds'] = embeds\n",
        "      df['x'] = umap_embeds[:,0]\n",
        "      df['y'] = umap_embeds[:,1]\n",
        "\n",
        "    # call the search Function\n",
        "    query = st.text_input(label='Search query', value='Show me something important')\n",
        "    df = search(df, query)\n",
        "\n",
        "    # Plot\n",
        "    chart = alt.Chart(df).transform_calculate(\n",
        "        url= alt.datum.id\n",
        "    ).mark_circle(size=60, stroke='#666', strokeWidth=1, opacity=0.3).encode(\n",
        "        x=#'x',\n",
        "        alt.X('x',\n",
        "            scale=alt.Scale(zero=False),\n",
        "            axis=alt.Axis(labels=False, ticks=False, domain=False)\n",
        "        ),\n",
        "        y=\n",
        "        alt.Y('y',\n",
        "            scale=alt.Scale(zero=False),\n",
        "            axis=alt.Axis(labels=False, ticks=False, domain=False)\n",
        "        ),\n",
        "        color=alt.Color('relevance', scale=alt.Scale(domain=[0, 1], range=['blue', 'red'])),\n",
        "        tooltip=['title']\n",
        "    ).properties(\n",
        "        width=800,\n",
        "        height=500\n",
        "    ).configure_legend(labelLimit= 0).configure_view(\n",
        "        strokeWidth=0\n",
        "    ).configure(background=\"#FAFAFA\").interactive()\n",
        "    st.altair_chart(chart, use_container_width=True)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-5HbXONi5_n",
        "outputId": "016698a6-85cf-4fd2-d6a7-de122b9318ee"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Streamlit App"
      ],
      "metadata": {
        "id": "BY9cLf68Iev5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get authtoken from https://dashboard.ngrok.com/get-started/setup \n",
        "# TODO: insert valid authtoken\n",
        "!ngrok authtoken 2EGJ6BBj3YLPwKOmEih64tTW8Ix_2bY2s98krjdTQmjLgNYFf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNkVe8Psi94n",
        "outputId": "2288a1b5-02bc-4066-a0bc-14be858c50e6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: opening the ngrok port in the browser returns unsecure side warning\n",
        "from pyngrok import ngrok\n",
        "public_url = ngrok.connect(port=8090)\n",
        "print(public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BOPqH3Si_RB",
        "outputId": "51e5853c-b67e-43cf-951e-2d31bb63ef78"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "http://b87f-35-199-175-165.ngrok.io\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run --server.port 8090 app.py &>/dev/null&"
      ],
      "metadata": {
        "id": "QCQWcBePjHYB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Close Down Streamlit App"
      ],
      "metadata": {
        "id": "RkHDMtKWIhL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# kill tunnel\n",
        "ngrok.kill()\n",
        "ngrok.disconnect(public_url)"
      ],
      "metadata": {
        "id": "rwl5EZb9jYg6"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# kill streamlit\n",
        "!pgrep streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DH-HntW8lVHR",
        "outputId": "8c953aa9-d72f-4e9a-e172-4ab41e5a990a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "271\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: insert streamlit process number\n",
        "!kill 271"
      ],
      "metadata": {
        "id": "vJeU_egqIocE"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h9_EpWPzkhFw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}