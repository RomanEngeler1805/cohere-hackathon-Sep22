{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIYdn1woOS1n",
        "outputId": "ea415a70-fc8a-4fdd-b407-4aa0791305b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting cohere\n",
            "  Downloading cohere-2.2.5.tar.gz (9.3 kB)\n",
            "Collecting umap-learn\n",
            "  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n",
            "\u001b[K     |████████████████████████████████| 88 kB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: altair in /usr/local/lib/python3.7/dist-packages (4.2.0)\n",
            "Collecting annoy\n",
            "  Downloading annoy-1.17.1.tar.gz (647 kB)\n",
            "\u001b[K     |████████████████████████████████| 647 kB 11.1 MB/s \n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-2.4.0-py3-none-any.whl (365 kB)\n",
            "\u001b[K     |████████████████████████████████| 365 kB 40.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.0)\n",
            "Collecting bertopic\n",
            "  Downloading bertopic-0.11.0-py2.py3-none-any.whl (76 kB)\n",
            "\u001b[K     |████████████████████████████████| 76 kB 3.2 MB/s \n",
            "\u001b[?25hCollecting transformers\n",
            "  Downloading transformers-4.21.2-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 70.7 MB/s \n",
            "\u001b[?25hCollecting streamlit\n",
            "  Downloading streamlit-1.12.2-py2.py3-none-any.whl (9.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.1 MB 37.0 MB/s \n",
            "\u001b[?25hCollecting pyngrok==4.1.1\n",
            "  Downloading pyngrok-4.1.1.tar.gz (18 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyngrok==4.1.1) (0.16.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyngrok==4.1.1) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from cohere) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.7.3)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (0.56.0)\n",
            "Collecting pynndescent>=0.5\n",
            "  Downloading pynndescent-0.5.7.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 14.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn) (0.39.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn) (4.12.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from pynndescent>=0.5->umap-learn) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->umap-learn) (3.1.0)\n",
            "Requirement already satisfied: pandas>=0.18 in /usr/local/lib/python3.7/dist-packages (from altair) (1.3.5)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from altair) (2.11.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair) (0.12.0)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.7/dist-packages (from altair) (4.3.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair) (22.1.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair) (5.9.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair) (4.1.1)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair) (0.18.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=3.0->altair) (3.8.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.18->altair) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.18->altair) (2022.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.18->altair) (1.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.7.1)\n",
            "Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Collecting huggingface-hub<1.0.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n",
            "\u001b[K     |████████████████████████████████| 120 kB 62.3 MB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 63.6 MB/s \n",
            "\u001b[?25hCollecting multiprocess\n",
            "  Downloading multiprocess-0.70.13-py37-none-any.whl (115 kB)\n",
            "\u001b[K     |████████████████████████████████| 115 kB 48.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->cohere) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->cohere) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->cohere) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->cohere) (2022.6.15)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 46.8 MB/s \n",
            "\u001b[?25hCollecting sentence-transformers>=0.4.1\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[K     |████████████████████████████████| 85 kB 5.4 MB/s \n",
            "\u001b[?25hCollecting PyYAML\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 45.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.7/dist-packages (from bertopic) (5.5.0)\n",
            "Collecting hdbscan>=0.8.28\n",
            "  Downloading hdbscan-0.8.28.tar.gz (5.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.2 MB 35.6 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cython>=0.27 in /usr/local/lib/python3.7/dist-packages (from hdbscan>=0.8.28->bertopic) (0.29.32)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly>=4.7.0->bertopic) (8.0.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.4.1->bertopic) (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.13.1+cu113)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.4.1->bertopic) (3.7)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 39.6 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 42.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: protobuf<4,>=3.12 in /usr/local/lib/python3.7/dist-packages (from streamlit) (3.17.3)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (7.1.2)\n",
            "Collecting watchdog\n",
            "  Downloading watchdog-2.1.9-py3-none-manylinux2014_x86_64.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 6.3 MB/s \n",
            "\u001b[?25hCollecting validators>=0.2\n",
            "  Downloading validators-0.20.0.tar.gz (30 kB)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.10.2)\n",
            "Collecting gitpython!=3.1.19\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 66.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tzlocal>=1.1 in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.5.1)\n",
            "Collecting semver\n",
            "  Downloading semver-2.13.0-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (5.1.1)\n",
            "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.2.4)\n",
            "Collecting pympler>=0.9\n",
            "  Downloading Pympler-1.0.1-py3-none-any.whl (164 kB)\n",
            "\u001b[K     |████████████████████████████████| 164 kB 70.7 MB/s \n",
            "\u001b[?25hCollecting pydeck>=0.1.dev5\n",
            "  Downloading pydeck-0.8.0b1-py2.py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 43.6 MB/s \n",
            "\u001b[?25hCollecting rich>=10.11.0\n",
            "  Downloading rich-12.5.1-py3-none-any.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 75.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (7.1.2)\n",
            "Collecting blinker>=1.0.0\n",
            "  Downloading blinker-1.5-py2.py3-none-any.whl (12 kB)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.8 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->altair) (2.0.1)\n",
            "Collecting commonmark<0.10.0,>=0.9.0\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 6.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from rich>=10.11.0->streamlit) (2.6.1)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from validators>=0.2->streamlit) (4.4.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.8.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Building wheels for collected packages: pyngrok, cohere, umap-learn, pynndescent, annoy, hdbscan, sentence-transformers, validators\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-4.1.1-py3-none-any.whl size=15983 sha256=5d847add0a4b0c5df4ff3b62f37f446a5bcbaf96f94684e50ad4468a6acd4b6c\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/d9/12/045a042fee3127dc40ba6f5df2798aa2df38c414bf533ca765\n",
            "  Building wheel for cohere (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cohere: filename=cohere-2.2.5-cp37-cp37m-linux_x86_64.whl size=10453 sha256=9da0404e234a11233f4e753cdf2e067d8ddfb9999c376d55adf2ce977a187d30\n",
            "  Stored in directory: /root/.cache/pip/wheels/eb/19/2d/89553daf06bc949f1aeb4cfae6e1e306d7763b22ffe91e45fa\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82829 sha256=c792c38760008d43eb556800e7b12deee15aa47400c88799f14f25136c5fdbdf\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/52/a5/1fd9e3e76a7ab34f134c07469cd6f16e27ef3a37aeff1fe821\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.7-py3-none-any.whl size=54286 sha256=7231cfdbae7cd466db13ca6f3e9fb9f32ae27446af70291f3ef376c8855d483c\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/2a/f8/7bd5dcec71bd5c669f6f574db3113513696b98f3f9b51f496c\n",
            "  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for annoy: filename=annoy-1.17.1-cp37-cp37m-linux_x86_64.whl size=395180 sha256=cdac424b1f688801f68e62dc6c0f6ee959d82553bfc175867842f64318445998\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/94/bf/92cb0e4fef8770fe9c6df0ba588fca30ab7c306b6048ae8a54\n",
            "  Building wheel for hdbscan (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdbscan: filename=hdbscan-0.8.28-cp37-cp37m-linux_x86_64.whl size=2340276 sha256=05b3b8fec14193e1439008515e8bbfe08a74cac8c1d73244e43117bae2b05694\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/7a/5e/259ccc841c085fc41b99ef4a71e896b62f5161f2bc8a14c97a\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=add259b6f459a88ac6f071f4ce8b4d5a42552ef71ecd66312e469be80c6a5d49\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/06/fb/d59c1e5bd1dac7f6cf61ec0036cc3a10ab8fecaa6b2c3d3ee9\n",
            "  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for validators: filename=validators-0.20.0-py3-none-any.whl size=19582 sha256=5e7a15a331d1d950fe0449a40819dc8287a9a4f16225b0ab437d25b92e8eb62c\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/55/ab/36a76989f7f88d9ca7b1f68da6d94252bb6a8d6ad4f18e04e9\n",
            "Successfully built pyngrok cohere umap-learn pynndescent annoy hdbscan sentence-transformers validators\n",
            "Installing collected packages: urllib3, PyYAML, tokenizers, smmap, huggingface-hub, transformers, sentencepiece, pynndescent, gitdb, commonmark, xxhash, watchdog, validators, umap-learn, sentence-transformers, semver, rich, responses, pympler, pydeck, multiprocess, hdbscan, gitpython, blinker, streamlit, pyngrok, datasets, cohere, bertopic, annoy\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 6.0\n",
            "    Uninstalling PyYAML-6.0:\n",
            "      Successfully uninstalled PyYAML-6.0\n",
            "Successfully installed PyYAML-5.4.1 annoy-1.17.1 bertopic-0.11.0 blinker-1.5 cohere-2.2.5 commonmark-0.9.1 datasets-2.4.0 gitdb-4.0.9 gitpython-3.1.27 hdbscan-0.8.28 huggingface-hub-0.9.1 multiprocess-0.70.13 pydeck-0.8.0b1 pympler-1.0.1 pyngrok-4.1.1 pynndescent-0.5.7 responses-0.18.0 rich-12.5.1 semver-2.13.0 sentence-transformers-2.2.2 sentencepiece-0.1.97 smmap-5.0.0 streamlit-1.12.2 tokenizers-0.12.1 transformers-4.21.2 umap-learn-0.5.3 urllib3-1.25.11 validators-0.20.0 watchdog-2.1.9 xxhash-3.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install cohere umap-learn altair annoy datasets tqdm bertopic transformers datasets streamlit pyngrok==4.1.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ckNnprty4iTB"
      },
      "outputs": [],
      "source": [
        "import cohere\n",
        "import numpy as np\n",
        "import re\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "import umap\n",
        "import altair as alt\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from annoy import AnnoyIndex\n",
        "import warnings\n",
        "from sklearn.cluster import KMeans\n",
        "from bertopic._ctfidf import ClassTFIDF\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_colwidth', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4M53MNVsxex",
        "outputId": "d5d27ff7-e4d9-4dc7-e02d-34ee304fd78e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting helper.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile helper.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import cohere\n",
        "import umap\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import warnings\n",
        "from sklearn.cluster import KMeans\n",
        "from bertopic._ctfidf import ClassTFIDF\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "\n",
        "api_key = 'WKsYz7mFBHEDsI7xsZ9KJP3WPMm5txIjKmU0eBTK'\n",
        "co = cohere.Client(api_key)\n",
        "title = 'Some title needs to be inputted here'\n",
        "\n",
        "@st.cache(allow_output_mutation=True)\n",
        "def get_dataset(df, text, title):\n",
        "  max_length = 500\n",
        "  df.rename(columns={text: 'text', title: 'title'}, inplace=True)\n",
        "  df = df[['title', 'text']]\n",
        "  max_length = min(max_length, df.shape[0])\n",
        "  df = df.head(max_length)\n",
        "  return df\n",
        "\n",
        "@st.cache(allow_output_mutation=True)\n",
        "def get_embeddings(df):\n",
        "    embeds = co.embed(texts=list(df['text']),\n",
        "                    model='large',\n",
        "                    truncate='LEFT').embeddings\n",
        "    reducer = umap.UMAP(n_neighbors=100) \n",
        "    umap_embeds = reducer.fit_transform(embeds)\n",
        "    return (embeds, umap_embeds)\n",
        "\n",
        "@st.cache(allow_output_mutation=True)\n",
        "def get_keywords(df, n_clusters=8, chart_title='This is the title'):\n",
        "  embeds, umap_embeds = get_embeddings(df)\n",
        "  df['x'] = umap_embeds[:,0]\n",
        "  df['y'] = umap_embeds[:,1]\n",
        "\n",
        "  kmeans_model = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "  classes = kmeans_model.fit_predict(embeds)\n",
        "  documents =  df['title']\n",
        "  documents = pd.DataFrame({\"Document\": documents,\n",
        "                            \"ID\": range(len(documents)),\n",
        "                            \"Topic\": None})\n",
        "  documents['Topic'] = classes\n",
        "  documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
        "  count_vectorizer = CountVectorizer(stop_words=\"english\").fit(documents_per_topic.Document)\n",
        "  count = count_vectorizer.transform(documents_per_topic.Document)\n",
        "  words = count_vectorizer.get_feature_names()\n",
        "  ctfidf = ClassTFIDF().fit_transform(count).toarray()\n",
        "  words_per_class = {label: [words[index] for index in ctfidf[label].argsort()[-10:]] for label in documents_per_topic.Topic}\n",
        "  df['cluster'] = classes\n",
        "  df['keywords'] = df['cluster'].map(lambda topic_num: \", \".join(np.array(words_per_class[topic_num])[:]))\n",
        "  return df, chart_title"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile semantic_search.py\n",
        "import streamlit as st\n",
        "import cohere\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from annoy import AnnoyIndex\n",
        "from helper import get_dataset, get_embeddings, get_keywords\n",
        "\n",
        "@st.cache(allow_output_mutation=True)\n",
        "def search(df, query):\n",
        "  api_key = 'dwhPny8kTpkhDNpu05484MtqjFU2QKXeYx9kH6DA'\n",
        "  co = cohere.Client(api_key)\n",
        "  title = 'Some title needs to be inputted here'\n",
        "\n",
        "  # Get embeddings\n",
        "  embeds, umap_embeds = get_embeddings(df)\n",
        "  df['x'] = umap_embeds[:,0]\n",
        "  df['y'] = umap_embeds[:,1]\n",
        "\n",
        "  # query and embedding\n",
        "  temp_dict = {'text': query}\n",
        "  df_query = pd.DataFrame(temp_dict, index=[0])\n",
        "\n",
        "  # embed query\n",
        "  query_embed, query_umap_embed = get_embeddings(df_query)\n",
        "\n",
        "  # create search index\n",
        "  embeds = np.array(embeds)\n",
        "\n",
        "  search_index = AnnoyIndex(embeds.shape[1], 'angular')\n",
        "  # Add all the vectors to the search index\n",
        "  for i in range(len(embeds)):\n",
        "      search_index.add_item(i, embeds[i])\n",
        "\n",
        "  search_index.build(10) # 10 trees\n",
        "  search_index.save('test.ann')\n",
        "\n",
        "  # Retrieve the nearest neighbors\n",
        "  similar_item_ids = search_index.get_nns_by_vector(query_embed[0],10,\n",
        "                                                  include_distances=True)\n",
        "  # Format the results\n",
        "  results = pd.DataFrame(data={'texts': df.iloc[similar_item_ids[0]]['text'],\n",
        "                              'distance': similar_item_ids[1]})\n",
        "\n",
        "  # find neighbours\n",
        "  neighbour = []\n",
        "\n",
        "  for k in range(len(df)):\n",
        "    if k in similar_item_ids[0]:\n",
        "      neighbour.append(1)\n",
        "    else:\n",
        "      neighbour.append(0)\n",
        "\n",
        "  df_neighbour = pd.DataFrame(neighbour, columns=['neighbour'])\n",
        "\n",
        "  df = df.join(df_neighbour);\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "s7x0K0Cq1HcA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bec531f4-fe08-4676-cb59-7fda011c6e99"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting semantic_search.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeRLX4UTvJPk",
        "outputId": "9391656e-47e9-4be8-d083-02007211078c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from datasets import load_dataset\n",
        "import cohere\n",
        "import numpy as np\n",
        "import re\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "import umap\n",
        "import altair as alt\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from annoy import AnnoyIndex\n",
        "import warnings\n",
        "from sklearn.cluster import KMeans\n",
        "from bertopic._ctfidf import ClassTFIDF\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from PIL import Image\n",
        "import requests\n",
        "from helper import get_dataset, get_embeddings, get_keywords\n",
        "from semantic_search import search\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn import metrics\n",
        "from scipy.spatial.distance import cdist\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "\n",
        "api_key = 'WKsYz7mFBHEDsI7xsZ9KJP3WPMm5txIjKmU0eBTK'\n",
        "co = cohere.Client(api_key)\n",
        "title = 'Some title needs to be inputted here'\n",
        "\n",
        "\n",
        "\n",
        "@st.cache(allow_output_mutation=True)\n",
        "def get_keywords(df, n_clusters=8, chart_title='This is the title'):\n",
        "  def get_embeddings(df):\n",
        "    embeds = co.embed(texts=list(df['text']),\n",
        "                    model='large',\n",
        "                    truncate='LEFT').embeddings\n",
        "    reducer = umap.UMAP(n_neighbors=100) \n",
        "    umap_embeds = reducer.fit_transform(embeds)\n",
        "    return (embeds, umap_embeds)\n",
        "\n",
        "  embeds, umap_embeds = get_embeddings(df)\n",
        "  df['x'] = umap_embeds[:,0]\n",
        "  df['y'] = umap_embeds[:,1]\n",
        "\n",
        "  kmeans_model = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "  classes = kmeans_model.fit_predict(embeds)\n",
        "  documents =  df['title']\n",
        "  documents = pd.DataFrame({\"Document\": documents,\n",
        "                            \"ID\": range(len(documents)),\n",
        "                            \"Topic\": None})\n",
        "  documents['Topic'] = classes\n",
        "  documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
        "  count_vectorizer = CountVectorizer(stop_words=\"english\").fit(documents_per_topic.Document)\n",
        "  count = count_vectorizer.transform(documents_per_topic.Document)\n",
        "  words = count_vectorizer.get_feature_names()\n",
        "  ctfidf = ClassTFIDF().fit_transform(count).toarray()\n",
        "  words_per_class = {label: [words[index] for index in ctfidf[label].argsort()[-10:]] for label in documents_per_topic.Topic}\n",
        "  df['cluster'] = classes\n",
        "  df['keywords'] = df['cluster'].map(lambda topic_num: \", \".join(np.array(words_per_class[topic_num])[:]))\n",
        "  return df, chart_title\n",
        "\n",
        "def main():\n",
        "\n",
        "  df = pd.DataFrame({'title': [], 'text': []})\n",
        "  # title\n",
        "  col1, _, col2 = st.columns([1,1,15])\n",
        "\n",
        "  image_cohere = Image.open(requests.get('https://avatars.githubusercontent.com/u/54850923?s=280&v=4', stream=True).raw)\n",
        "  col1.image(image_cohere, width=80)\n",
        "  #\n",
        "  col2.title('Analyze')\n",
        "  #\n",
        "  app_mode = st.sidebar.selectbox('Task', ['Import', 'EDA', 'Cluster', 'Search'])\n",
        "\n",
        "  with st.expander(\"How this works.\", expanded=False):\n",
        "    st.write(\n",
        "      \"\"\"     \n",
        "      Embeddings are hard to vizualize. Analyze makes it a breeze.\n",
        "      1. Go ahead and upload a csv file that you want to examine.\n",
        "          The csv file needs to have atleast 2 columns:\n",
        "          - The first column being shorter text - a title for example.\n",
        "          - The second column being the longer text - the body of the text for example.\n",
        "      2. You then have 3 options:\n",
        "          - EDA: Get an overview of the file and get some general exploratory data analysis.\n",
        "          - Cluster: Do some cluster analysis, with keywords generated from the body of the text and using the titles.\n",
        "          - Search: Query the data and retrieve the closest match.\n",
        "\n",
        "      To make sure this app works quickly, we only capture the first 500 lines of text.     \n",
        "      \"\"\"\n",
        "    )\n",
        "    st.markdown('')\n",
        "  uploaded_file = st.file_uploader(\"Choose a file\")\n",
        "  if uploaded_file is not None:\n",
        "    df = pd.read_csv(uploaded_file, usecols=[0, 1])\n",
        "\n",
        "  \n",
        "   \n",
        "  if app_mode == \"Import\":\n",
        "      st.markdown('')\n",
        "\n",
        "      st.markdown(\n",
        "          \"\"\"\n",
        "          <style>\n",
        "          [data-testid=\"stSidebar\"][aria-expanded=\"true\"] > div:first-child{\n",
        "              width: 350px\n",
        "          }\n",
        "          [data-testid=\"stSidebar\"][aria-expanded=\"false\"] > div:first-child{\n",
        "              width: 350px\n",
        "              margin-left: -350px\n",
        "          }\n",
        "          </style>\n",
        "          \"\"\",\n",
        "\n",
        "          unsafe_allow_html=True,\n",
        "      )\n",
        "      #with st.expander(\"Help\", expanded=False):\n",
        "      #  st.write(\n",
        "      #    \"\"\"     \n",
        "      #    Embeddings are hard to vizualize. Analyze makes it a breeze.\n",
        "      #    1. Go ahead and upload a csv file that you want to examine.\n",
        "      #       The csv file needs to have atleast 2 columns:\n",
        "      #       - The first column being shorter text - a title for example.\n",
        "      #       - The second column being the longer text - the body of the text for example.\n",
        "      #    2. You then have 3 options:\n",
        "      #       - EDA: Get an overview of the file and get some general exploratory data analysis.\n",
        "      #       - Cluster: Do some cluster analysis, with keywords generated from the body of the text and using the titles.\n",
        "      #       - Search: Query the data and retrieve the closest match.\n",
        "      #\n",
        "      #    To make sure this app works quickly, we only capture the first 500 lines of text.     \n",
        "      #    \"\"\"\n",
        "      #)\n",
        "      #st.markdown('')\n",
        "      #uploaded_file = st.file_uploader(\"Choose a file\")\n",
        "      #if uploaded_file is not None:\n",
        "      #  df = pd.read_csv(uploaded_file, usecols=[0, 1])\n",
        "      #  st.write(uploaded_file.name)\n",
        "      #  st.write(dataframe)\n",
        "  elif app_mode == 'EDA':\n",
        "    st.sidebar.subheader(' Quick  Explore')\n",
        "    st.markdown(\"Tick the box on the side panel to explore the dataset.\")\n",
        "    if st.sidebar.checkbox('Basic Info'):\n",
        "        if st.sidebar.checkbox(\"Show Columns\"):\n",
        "            st.subheader('Show Columns List')\n",
        "            all_columns = df.columns.to_list()\n",
        "            st.write(all_columns)\n",
        "\n",
        "        if st.sidebar.checkbox('Overview'):\n",
        "            st.subheader('File contents')\n",
        "            st.write(df)\n",
        "            st.write(f'The number of lines is {df.shape[0]}. We will only process {min(500, df.shape[0])}')\n",
        "        if st.sidebar.checkbox('Missing Values?'):\n",
        "            st.subheader('Missing values')\n",
        "            st.write(df.isnull().sum())\n",
        "\n",
        "  elif app_mode == 'Cluster':\n",
        "    df.columns = ['title', 'text']\n",
        "    embeds, umap_embeds = get_embeddings(df)\n",
        "    low, med, high = 1, 8, 10\n",
        "\n",
        "    with st.expander(\"Help\", expanded=False):\n",
        "      st.write(\n",
        "          \"\"\"     \n",
        "          One of the ways to determine the optimal number of clusters, is to choose the number corresponding to an elbow, if it exists.\n",
        "          \"\"\"\n",
        "      )\n",
        "      st.markdown(\"\")\n",
        "      distortions = []\n",
        "      nembeds = np.array(embeds)\n",
        "      for k in range(low, high + 1):\n",
        "        km = KMeans(n_clusters=k)\n",
        "        km.fit(nembeds)\n",
        "        distortions.append(sum(np.min(cdist(nembeds, km.cluster_centers_,\n",
        "                                              'euclidean'), axis=1)) / nembeds.shape[0])  \n",
        "\n",
        "      fig = plt.figure(figsize=(10, 4))\n",
        "      plt.plot(range(low, high + 1), distortions, 'bx-')\n",
        "      plt.xlabel('Number of clusters')\n",
        "      plt.ylabel('Distortion')\n",
        "      plt.title('Determine the optimal number of clusters')\n",
        "      st.pyplot(fig)\n",
        "\n",
        "    n_clusters = st.slider('Select number of clusters', low, high, med)\n",
        "    #st.write('Number of clusters:', n_clusters)\n",
        "    #df.columns = ['title', 'text']\n",
        "    #st.write(df.head())\n",
        "    df = get_dataset(df, text='text', title='title')\n",
        "    try:\n",
        "      chart_title = uploaded_file.name.split('.')[0]\n",
        "    except:\n",
        "      chart_title = 'Title TBD'\n",
        "    df, chart_title = get_keywords(df, n_clusters=n_clusters, chart_title=chart_title)\n",
        "    selection = alt.selection_multi(fields=['keywords'], bind='legend')\n",
        "    chart = alt.Chart(df).transform_calculate(\n",
        "        url=alt.datum.id\n",
        "    ).mark_circle(size=60, stroke='#666', strokeWidth=1, opacity=0.3).encode(\n",
        "        x=#'x',\n",
        "        alt.X('x',\n",
        "            scale=alt.Scale(zero=False),\n",
        "            axis=alt.Axis(labels=False, ticks=False, domain=False)\n",
        "        ),\n",
        "        y=\n",
        "        alt.Y('y',\n",
        "            scale=alt.Scale(zero=False),\n",
        "            axis=alt.Axis(labels=False, ticks=False, domain=False)\n",
        "        ),\n",
        "        href='url:N',\n",
        "        color=alt.Color('keywords:N', \n",
        "                        legend=alt.Legend(columns=1, symbolLimit=0, labelFontSize=14)\n",
        "                      ),\n",
        "        opacity=alt.condition(selection, alt.value(1), alt.value(0.2)),\n",
        "        tooltip=['title', 'keywords', 'cluster']\n",
        "    ).properties(\n",
        "        width=800,\n",
        "        height=500\n",
        "    ).add_selection(\n",
        "        selection\n",
        "    ).configure_legend(labelLimit= 0).configure_view(\n",
        "        strokeWidth=0\n",
        "    ).configure(background=\"#FAFAFA\").properties(\n",
        "        title=chart_title\n",
        "    ).interactive()\n",
        "    st.altair_chart(chart, use_container_width=True)\n",
        "  elif app_mode == 'Search':\n",
        "    # call the search Function\n",
        "    # TODO: pass query from here\n",
        "    # call the search Function\n",
        "    query = st.text_input(label='Search query', value='Show me something important')\n",
        "    #st.write(df.head())\n",
        "    df.columns = ['title', 'text']\n",
        "    df = search(df, query)\n",
        "    # Plot\n",
        "    chart = alt.Chart(df).transform_calculate(\n",
        "        url= alt.datum.id\n",
        "    ).mark_circle(size=60, stroke='#666', strokeWidth=1, opacity=0.3).encode(\n",
        "        x=#'x',\n",
        "        alt.X('x',\n",
        "            scale=alt.Scale(zero=False),\n",
        "            axis=alt.Axis(labels=False, ticks=False, domain=False)\n",
        "        ),\n",
        "        y=\n",
        "        alt.Y('y',\n",
        "            scale=alt.Scale(zero=False),\n",
        "            axis=alt.Axis(labels=False, ticks=False, domain=False)\n",
        "        ),\n",
        "        color=alt.Color('neighbour', scale=alt.Scale(domain=[0, 1], range=['blue', 'red'])),\n",
        "        tooltip=['title']\n",
        "    ).properties(\n",
        "        width=800,\n",
        "        height=500\n",
        "    ).configure_legend(labelLimit= 0).configure_view(\n",
        "        strokeWidth=0\n",
        "    ).configure(background=\"#FAFAFA\")\n",
        "    #chart.interactive()\n",
        "    st.altair_chart(chart.interactive(), use_container_width=True)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEWxRiAO-Abd",
        "outputId": "7dd54c0b-1c76-4a3e-9e67-ef52ddf4ee4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!ngrok authtoken 2EFrz6ZHXEJzLa2gEluLjJONyo6_2T1iQUfKihu5g8gftsLDw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZLkJBdllehV",
        "outputId": "c98764f0-8ef2-471c-d823-a53563731c20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    PID TTY          TIME CMD\n",
            "      1 ?        00:00:00 docker-init\n",
            "      8 ?        00:00:08 node\n",
            "     18 ?        00:00:00 tail\n",
            "     31 ?        00:00:05 python3 <defunct>\n",
            "     32 ?        00:00:00 colab-fileshim.\n",
            "     45 ?        00:00:05 jupyter-noteboo\n",
            "     46 ?        00:00:09 dap_multiplexer\n",
            "     66 ?        00:01:10 python3\n",
            "     88 ?        00:00:17 python3\n",
            "    288 ?        00:05:35 node\n",
            "   1865 ?        00:03:58 streamlit\n",
            "   1973 ?        00:00:00 ps\n"
          ]
        }
      ],
      "source": [
        "!ps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9H24UZUB3Jt",
        "outputId": "21d8272f-a8f6-41d2-83a5-55f378934032"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "http://66a8-34-66-124-73.ngrok.io\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "public_url = ngrok.connect(port=8090)\n",
        "print(public_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-GWHZYQ9Cz8",
        "outputId": "7bf3fa78-7ace-4778-ed68-e4d3f5844ea3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-09-04 18:57:38.887 INFO    numexpr.utils: NumExpr defaulting to 4 threads.\n"
          ]
        }
      ],
      "source": [
        "!streamlit run --server.port 8090 app.py >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "f5AS-O7EyTIC",
        "outputId": "eb6e5028-2f5a-4cbe-e833-f4903818ffd6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "app.py\thelper.py  __pycache__\tsample_data  semantic_search.py  test.ann\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('semantic_search.py')"
      ],
      "metadata": {
        "id": "wUIOlVnpyWjd",
        "outputId": "ee2c5f02-d77a-4d11-9eb1-d5316af030af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5cd0ab0c-f10d-4d22-93d3-e95556b9362b\", \"semantic_search.py\", 1608)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0Df5mA42yfUO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}